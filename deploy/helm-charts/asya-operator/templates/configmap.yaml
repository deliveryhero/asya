apiVersion: v1
kind: ConfigMap
metadata:
  name: test-handlers
  namespace: asya
data:
  gateway_handlers.py: |
    """
    Gateway test handlers with heartbeat support.

    These handlers send progress updates (heartbeats) to the gateway
    to test the SSE progress streaming functionality.
    """

    import os
    import time
    from typing import Any, Dict

    import requests


    # Gateway configuration
    ASYA_GATEWAY_URL = os.getenv("ASYA_GATEWAY_URL", "http://gateway:8080")
    HEARTBEAT_URL = f"{ASYA_GATEWAY_URL}/jobs"


    def send_heartbeat(job_id: str, actor_name: str, status: str, message: str = ""):
        """Send a heartbeat update to the gateway."""
        try:
            heartbeat = {
                "actor_name": actor_name,
                "status": status,  # "picked_up", "processing", "completed", "error"
                "message": message,
            }

            response = requests.post(
                f"{HEARTBEAT_URL}/{job_id}/heartbeat",
                json=heartbeat,
                timeout=5,
            )

            if response.status_code != 200:
                print(f"Heartbeat failed: {response.status_code} - {response.text}")

        except Exception as e:
            print(f"Failed to send heartbeat: {e}")


    def extract_job_id(msg: Dict[str, Any]) -> str:
        """Extract job ID from route metadata if present."""
        route = msg.get("route", {})
        metadata = route.get("metadata", {})
        return metadata.get("job_id", "")


    def echo_handler(msg: Dict[str, Any]) -> Dict[str, Any]:
        """
        Simple echo handler that returns the input message.

        Sends heartbeats: picked_up → processing → completed
        """
        job_id = extract_job_id(msg)
        actor_name = "echo-actor"

        # Send picked_up heartbeat
        if job_id:
            send_heartbeat(job_id, actor_name, "picked_up")

        payload = msg.get("payload", {})
        message = payload.get("message", "")

        # Send processing heartbeat
        if job_id:
            send_heartbeat(job_id, actor_name, "processing", "Echoing message...")

        # Simulate processing time for SSE heartbeat testing
        time.sleep(0.5)

        # Send completed heartbeat
        if job_id:
            send_heartbeat(job_id, actor_name, "completed")

        return {
            "status": "processed",
            "handler": "echo",
            "echoed": message,
        }


    def progress_handler(msg: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handler that simulates progress with multiple heartbeat updates.

        Sends multiple processing updates to test SSE streaming.
        """
        job_id = extract_job_id(msg)
        actor_name = "progress-actor"

        print(f"[HANDLER] progress_handler started: job_id={job_id}")

        # Send picked_up
        if job_id:
            print(f"[HANDLER] Sending picked_up heartbeat: job_id={job_id}")
            send_heartbeat(job_id, actor_name, "picked_up")

        payload = msg.get("payload", {})
        steps = payload.get("steps", 3)
        print(f"[HANDLER] Processing {steps} steps")

        # Send progress updates for each step
        for i in range(steps):
            if job_id:
                percent = int(((i + 1) / steps) * 100)
                print(f"[HANDLER] Sending processing heartbeat: job_id={job_id} step={i+1}/{steps} percent={percent}%")
                send_heartbeat(
                    job_id,
                    actor_name,
                    "processing",
                    f"Processing step {i+1}/{steps} ({percent}%)",
                )
            time.sleep(0.3)  # Delay between progress updates for testing

        # Send completed
        if job_id:
            print(f"[HANDLER] Sending completed heartbeat: job_id={job_id}")
            send_heartbeat(job_id, actor_name, "completed")

        result = {
            "status": "completed",
            "steps_completed": steps,
        }
        print(f"[HANDLER] progress_handler finished: job_id={job_id} result={result}")
        return result


    def pipeline_doubler(msg: Dict[str, Any]) -> Dict[str, Any]:
        """
        First step in pipeline: doubles the input value.

        Part of test_pipeline test.
        """
        job_id = extract_job_id(msg)
        actor_name = "doubler-actor"

        print(f"[HANDLER] pipeline_doubler started: job_id={job_id}")

        if job_id:
            print(f"[HANDLER] Sending picked_up heartbeat: job_id={job_id}")
            send_heartbeat(job_id, actor_name, "picked_up")

        payload = msg.get("payload", {})
        value = payload.get("value", 0)
        print(f"[HANDLER] Input value: {value}")

        if job_id:
            print(f"[HANDLER] Sending processing heartbeat: job_id={job_id}")
            send_heartbeat(job_id, actor_name, "processing", "Doubling value...")

        time.sleep(0.3)  # Simulate processing time for pipeline testing

        result = value * 2
        print(f"[HANDLER] Computed result: {value} * 2 = {result}")

        if job_id:
            print(f"[HANDLER] Sending completed heartbeat: job_id={job_id}")
            send_heartbeat(job_id, actor_name, "completed")

        output = {
            "value": result,
            "operation": "doubled",
        }
        print(f"[HANDLER] pipeline_doubler finished: job_id={job_id} output={output}")
        return output


    def pipeline_incrementer(msg: Dict[str, Any]) -> Dict[str, Any]:
        """
        Second step in pipeline: adds 5 to the value.

        Part of test_pipeline test.
        """
        job_id = extract_job_id(msg)
        actor_name = "incrementer-actor"

        print(f"[HANDLER] pipeline_incrementer started: job_id={job_id}")

        if job_id:
            print(f"[HANDLER] Sending picked_up heartbeat: job_id={job_id}")
            send_heartbeat(job_id, actor_name, "picked_up")

        payload = msg.get("payload", {})
        value = payload.get("value", 0)
        print(f"[HANDLER] Input value: {value}")

        if job_id:
            print(f"[HANDLER] Sending processing heartbeat: job_id={job_id}")
            send_heartbeat(job_id, actor_name, "processing", "Incrementing value...")

        time.sleep(0.3)  # Simulate processing time for pipeline testing

        result = value + 5
        print(f"[HANDLER] Computed result: {value} + 5 = {result}")

        if job_id:
            print(f"[HANDLER] Sending completed heartbeat: job_id={job_id}")
            send_heartbeat(job_id, actor_name, "completed")

        output = {
            "value": result,
            "operation": "incremented",
        }
        print(f"[HANDLER] pipeline_incrementer finished: job_id={job_id} output={output}")
        return output


    def error_handler(msg: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handler that fails intentionally to test error handling.

        Sends error heartbeat.
        """
        job_id = extract_job_id(msg)
        actor_name = "error-actor"

        if job_id:
            send_heartbeat(job_id, actor_name, "picked_up")

        payload = msg.get("payload", {})
        should_fail = payload.get("should_fail", False)

        if should_fail:
            if job_id:
                send_heartbeat(job_id, actor_name, "error", "Intentional test failure")
            raise ValueError("Intentional test failure")

        return {"status": "ok"}


    def timeout_handler(msg: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handler that sleeps for a long time to test timeout handling.
        """
        job_id = extract_job_id(msg)
        actor_name = "timeout-actor"

        if job_id:
            send_heartbeat(job_id, actor_name, "picked_up")

        payload = msg.get("payload", {})
        sleep_seconds = payload.get("sleep_seconds", 5)

        if job_id:
            send_heartbeat(job_id, actor_name, "processing", f"Sleeping for {sleep_seconds}s...")

        time.sleep(sleep_seconds)  # Simulate long operation to test timeout handling

        if job_id:
            send_heartbeat(job_id, actor_name, "completed")

        return {"status": "completed", "slept": sleep_seconds}
  asya_runtime.py: |
    #!/usr/bin/env python3
    """
    Asya Actor Runtime - Unix Socket Server

    Simplified runtime that calls a user-specified Python function.

    Environment Variables:
        ASYA_SOCKET_PATH: Path to Unix socket (default: /tmp/sockets/app.sock)
        ASYA_SOCKET_CHMOD: Socket permissions in octal (default: "0o660", empty = skip chmod)
        ASYA_HANDLER: Full function path (e.g., "foo.bar.predict")
        ASYA_INCLUDE_METADATA: Include route/metadata in msg (default: "false")

    Function Contract:
        def your_function(msg: dict) -> dict:
            - msg["payload"] is always a dict
            - If ASYA_INCLUDE_METADATA=true, msg also contains: {"route": ..., "payload": {...}}
            - Returns a dict result (can be single value or list for fan-out)
    """

    import gc
    import importlib
    import json
    import os
    import re
    import signal
    import socket
    import struct
    import sys
    import traceback

    # Configuration
    ASYA_HANDLER = os.getenv("ASYA_HANDLER", "")
    ASYA_SOCKET_PATH = os.getenv("ASYA_SOCKET_PATH", "/tmp/sockets/app.sock")
    ASYA_SOCKET_CHMOD = os.getenv("ASYA_SOCKET_CHMOD", "0o660")  # Empty string = skip chmod
    ASYA_CHUNK_SIZE = int(os.getenv("ASYA_CHUNK_SIZE", 4096))
    ASYA_INCLUDE_ROUTE_INFO = os.getenv("ASYA_INCLUDE_ROUTE_INFO", "false").lower() in (
        "true",
        "1",
        "yes",
    )
    ASYA_HANDLER_TIMEOUT = int(os.getenv("ASYA_HANDLER_TIMEOUT", 0))  # 0 = disabled
    ASYA_ENABLE_OOM_DETECTION = os.getenv("ASYA_ENABLE_OOM_DETECTION", "true").lower() in (
        "true",
        "1",
        "yes",
    )
    ASYA_CUDA_CLEANUP_ON_OOM = os.getenv("ASYA_CUDA_CLEANUP_ON_OOM", "true").lower() in (
        "true",
        "1",
        "yes",
    )

    # Try to import torch for CUDA OOM detection
    try:
        import torch

        TORCH_AVAILABLE = True
    except ImportError:
        TORCH_AVAILABLE = False


    def _err(msg):
        """Print error message to stderr."""
        print(msg, file=sys.stderr, flush=True)


    def _error_dict(code, message, exc=None, severity=None, retry_after=None):
        """Create standardized error response dict."""
        error = {"status": "error", "error": code, "message": message}
        if exc is not None:
            error["type"] = type(exc).__name__
        if severity is not None:
            error["severity"] = severity
        if retry_after is not None:
            error["retry_after"] = retry_after
        return error


    def _is_cuda_oom_error(exc):
        """Check if exception is a CUDA OOM error."""
        exc_name = type(exc).__name__
        exc_str = str(exc).lower()

        # Check for common CUDA OOM indicators
        return (
            "cuda" in exc_name.lower()
            or "outofmemoryerror" in exc_name.lower()
            or ("cuda" in exc_str and "memory" in exc_str)
            or "out of memory" in exc_str
        )


    def _cleanup_cuda_memory():
        """Clear CUDA cache if torch is available."""
        if not TORCH_AVAILABLE or not ASYA_CUDA_CLEANUP_ON_OOM:
            return False

        try:
            torch.cuda.empty_cache()
            _err("INFO: Cleared CUDA cache after OOM")
            return True
        except Exception as e:
            _err(f"WARNING: Failed to clear CUDA cache: {e}")
            return False


    def load_function():
        """Load the user function from ASYA_HANDLER env var."""
        if not ASYA_HANDLER:
            _err("FATAL: ASYA_HANDLER not set")
            sys.exit(1)

        # Validate ASYA_HANDLER format to prevent path traversal and injection attacks
        # Allows: letters, numbers, underscores, dots (standard Python module paths)
        # Example valid: "my_module.submodule.function_name"
        # Example invalid: "../etc/passwd", "os;rm -rf /", "__import__('os').system('cmd')"
        handler_pattern = re.compile(r"^[a-zA-Z_][a-zA-Z0-9_]*(\.[a-zA-Z_][a-zA-Z0-9_]*)+$")
        if not handler_pattern.match(ASYA_HANDLER):
            _err(f"FATAL: Invalid ASYA_HANDLER format: {ASYA_HANDLER}")
            _err(
                "Expected format: 'module.path.function_name' "
                "(letters, numbers, underscores, and dots only)"
            )
            sys.exit(1)

        # Parse "foo.bar.baz.func" -> module="foo.bar.baz", func="func"
        parts = ASYA_HANDLER.rsplit(".", 1)
        if len(parts) != 2:
            _err(f"FATAL: Invalid ASYA_HANDLER format: {ASYA_HANDLER}")
            _err("Expected format: 'module.path.function_name'")
            sys.exit(1)

        module_path, func_name = parts

        try:
            module = importlib.import_module(module_path)
            func = getattr(module, func_name)

            if not callable(func):
                raise TypeError(f"{ASYA_HANDLER} is not callable")

            print(f"Loaded function: {ASYA_HANDLER}", flush=True)
            return func

        except Exception as e:
            _err(f"FATAL: Failed to load {ASYA_HANDLER}: {e}")
            traceback.print_exc()
            sys.exit(1)


    def build_msg(raw_message):
        """Build the msg dict to pass to user function."""
        if ASYA_INCLUDE_ROUTE_INFO:
            # raw_message has full structure: {"route": ..., "payload": ...}
            return raw_message
        else:
            return {"payload": raw_message.get("payload")}


    def recv_exact(sock, n: int) -> bytes:
        """Read exactly n bytes from socket."""
        chunks = []
        remaining = n
        while remaining > 0:
            chunk = sock.recv(min(remaining, ASYA_CHUNK_SIZE))
            if not chunk:
                raise ConnectionError("Connection closed while reading")
            chunks.append(chunk)
            remaining -= len(chunk)
        return b"".join(chunks)


    def send_msg(sock, data: bytes):
        """Send message with length-prefix (4-byte big-endian uint32)."""
        length = struct.pack(">I", len(data))
        sock.sendall(length + data)


    def setup_socket(socket_path):
        """Initialize Unix socket server."""
        # Remove socket file if it exists
        try:
            os.unlink(socket_path)
        except OSError:
            if os.path.exists(socket_path):
                raise

        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        sock.bind(socket_path)
        sock.listen(5)

        # Apply chmod if configured (skip if ASYA_SOCKET_CHMOD is empty)
        if ASYA_SOCKET_CHMOD:
            mode = int(ASYA_SOCKET_CHMOD, 8)  # Parse octal string like "0o660"
            os.chmod(socket_path, mode)
            print(f"Socket permissions set to {ASYA_SOCKET_CHMOD}", flush=True)

        print(f"Socket server listening on {socket_path}", flush=True)
        return sock


    def handle_request(conn, func):
        """Handle a single request with length-prefix framing."""
        try:
            # Read 4-byte length prefix
            length_bytes = recv_exact(conn, 4)
            length = struct.unpack(">I", length_bytes)[0]

            # Read exact message payload
            data = recv_exact(conn, length)

            # Parse JSON message
            try:
                raw_message = json.loads(data.decode("utf-8"))
            except (json.JSONDecodeError, UnicodeDecodeError) as e:
                return _error_dict("invalid_json", str(e), e, severity="fatal")

            # Build msg dict and call user function
            try:
                msg = build_msg(raw_message)
                result = func(msg)
                return {"status": "ok", "result": result}

            except MemoryError as e:
                # RAM OOM error
                error_trace = traceback.format_exc()
                _err(f"ERROR: RAM OOM:\n{error_trace}")

                if ASYA_ENABLE_OOM_DETECTION:
                    # Trigger garbage collection
                    gc.collect()
                    _err("INFO: Triggered garbage collection after RAM OOM")

                    return _error_dict(
                        "oom_error",
                        f"Out of memory during processing: {str(e)}",
                        e,
                        severity="recoverable",
                        retry_after=30,
                    )
                else:
                    # Treat as regular processing error if detection disabled
                    return _error_dict("processing_error", str(e), e, severity="fatal")

            except Exception as e:
                error_trace = traceback.format_exc()
                _err(f"ERROR: {error_trace}")

                # Check for CUDA OOM
                if ASYA_ENABLE_OOM_DETECTION and _is_cuda_oom_error(e):
                    _err("INFO: Detected CUDA OOM error")
                    _cleanup_cuda_memory()

                    return _error_dict(
                        "cuda_oom_error",
                        f"CUDA out of memory: {str(e)}",
                        e,
                        severity="recoverable",
                        retry_after=60,
                    )

                # Regular processing error
                return _error_dict("processing_error", str(e), e, severity="fatal")

        except ConnectionError as e:
            return _error_dict("connection_error", str(e), e, severity="fatal")

        except Exception as e:
            error_trace = traceback.format_exc()
            _err(f"ERROR: Connection handling failed:\n{error_trace}")
            return _error_dict("connection_error", str(e), e, severity="fatal")


    def main():
        """Main entry point."""
        print("Asya Actor Runtime starting...", flush=True)
        print(f"Socket: {ASYA_SOCKET_PATH}", flush=True)
        print(
            f"Socket chmod: {ASYA_SOCKET_CHMOD if ASYA_SOCKET_CHMOD else 'disabled'}",
            flush=True,
        )
        print(f"Function: {ASYA_HANDLER}", flush=True)
        print(f"Include route info: {ASYA_INCLUDE_ROUTE_INFO}", flush=True)
        print(f"Handler timeout: {ASYA_HANDLER_TIMEOUT}s (0=disabled)", flush=True)
        print(f"OOM detection: {ASYA_ENABLE_OOM_DETECTION}", flush=True)
        print(f"CUDA cleanup on OOM: {ASYA_CUDA_CLEANUP_ON_OOM}", flush=True)
        print(f"Torch available: {TORCH_AVAILABLE}", flush=True)

        func = load_function()
        sock = setup_socket(ASYA_SOCKET_PATH)

        def cleanup_and_exit(signum=None, frame=None):
            """Clean up socket and exit."""
            if signum:
                _err(f"\nReceived signal {signum}, shutting down...")
            sock.close()
            try:
                os.unlink(ASYA_SOCKET_PATH)
            except OSError:
                pass
            if signum:
                sys.exit(0)

        signal.signal(signal.SIGTERM, cleanup_and_exit)
        signal.signal(signal.SIGINT, cleanup_and_exit)

        try:
            while True:
                try:
                    conn, _ = sock.accept()
                except (ConnectionAbortedError, OSError):
                    break

                try:
                    response = handle_request(conn, func)
                    response_data = json.dumps(response).encode("utf-8")
                    send_msg(conn, response_data)

                except BrokenPipeError:
                    _err("WARNING: Client disconnected")

                except Exception as e:
                    _err(f"CRITICAL: Failed to send response: {e}")

                finally:
                    conn.close()

        except KeyboardInterrupt:
            _err("\nShutting down gracefully...")
        except Exception as e:
            _err(f"FATAL: {e}")
            traceback.print_exc()
            sys.exit(1)
        finally:
            cleanup_and_exit()


    if __name__ == "__main__":
        main()
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: gateway-routes
  namespace: asya
data:
  routes.yaml: |
    # Gateway test tools configuration
    # Used by gateway E2E integration tests

    defaults:
      progress: true  # Enable SSE progress streaming for all tools
      timeout: 30     # 30 second timeout for test tools

    tools:
      # Simple echo tool - single actor
      - name: test_echo
        description: Echo back the input message
        parameters:
          message:
            type: string
            description: Message to echo
            required: true
        route: [test-echo-queue]

      # Progress streaming test - shows multiple heartbeat updates
      - name: test_progress
        description: Test SSE progress streaming with multiple updates
        parameters:
          steps:
            type: integer
            description: Number of progress steps to simulate
            default: 3
        route: [test-progress-queue]

      # Multi-step pipeline - tests routing through multiple actors
      - name: test_pipeline
        description: Test multi-step pipeline processing
        parameters:
          value:
            type: integer
            description: Value to process through pipeline
            required: true
        route: [test-doubler-queue, test-incrementer-queue]

      # Error handling test
      - name: test_error
        description: Test error handling and error heartbeats
        parameters:
          should_fail:
            type: boolean
            description: Whether the handler should fail
            default: true
        route: [test-error-queue]

      # Timeout test
      - name: test_timeout
        description: Test timeout handling
        parameters:
          sleep_seconds:
            type: integer
            description: How long to sleep (should exceed timeout)
            default: 60
        route: [test-timeout-queue]
        timeout: 10  # Short timeout for testing
